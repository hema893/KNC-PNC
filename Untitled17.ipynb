{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57301e00-cb4b-47bc-b2ae-3ab9a3c59be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                            KNN & PCA ASSISMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6310ed54-0d65-4c8d-a5b1-c79c0c56cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
    "classification and regression problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6347a5d5-7332-4cc9-9d43-fdfd50da11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    " **K-Nearest Neighbors (KNN)**\n",
    "\n",
    "K-Nearest Neighbors is a **supervised machine learning algorithm** used for both **classification** and **regression** tasks.\n",
    "It is a **non-parametric, instance-based (lazy learning)** method, meaning it does not explicitly learn a model during training but instead stores the data and makes predictions based on the similarity between new and existing points.\n",
    "\n",
    "---\n",
    "\n",
    "### **How it works**\n",
    "\n",
    "1. Choose a value of **K** (number of neighbors).\n",
    "2. Compute the **distance** (commonly Euclidean, Manhattan, or Minkowski) between the new data point and all points in the training set.\n",
    "3. Select the **K nearest neighbors** to the new data point.\n",
    "4. Make predictions based on those neighbors.\n",
    "\n",
    "---\n",
    "\n",
    "### **KNN in Classification**\n",
    "\n",
    "* Each neighbor \"votes\" for its class.\n",
    "* The class with the **majority vote** among the K neighbors is assigned to the new data point.\n",
    "\n",
    "**Example:**\n",
    "If K=5 and the nearest neighbors have classes `[Dog, Dog, Cat, Dog, Cat]`, the majority is **Dog**, so the new sample is classified as **Dog**.\n",
    "\n",
    "---\n",
    "\n",
    "### **KNN in Regression**\n",
    "\n",
    "* Instead of voting, KNN takes the **average (or weighted average)** of the neighbors’ values.\n",
    "* The predicted value is the mean of the target values of the K closest points.\n",
    "\n",
    "**Example:**\n",
    "If K=3 and the neighbors have house prices `[200k, 220k, 210k]`, the predicted price is **(200k + 220k + 210k)/3 = 210k**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points to Remember**\n",
    "\n",
    "* **Choice of K:**\n",
    "\n",
    "  * Small K → more sensitive to noise.\n",
    "  * Large K → smoother decision boundaries, but may overlook local patterns.\n",
    "* **Distance metric matters** (Euclidean for continuous data, Hamming for categorical, etc.).\n",
    "* **Feature scaling is important** (since distances are affected by feature magnitudes).\n",
    "* KNN can be **computationally expensive** for large datasets (since it requires storing and comparing with all data points).\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "\n",
    "* **Classification:** Majority vote of neighbors.\n",
    "* **Regression:** Average of neighbors’ values.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241120bf-54d6-4613-8290-71ba4d4371e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
    "performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f1694-8682-415d-ac34-af8cee0edaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### **Curse of Dimensionality**\n",
    "\n",
    "The *curse of dimensionality* refers to problems that arise when data has **too many features (dimensions)**.\n",
    "As the number of dimensions increases:\n",
    "\n",
    "* Data becomes **sparse** (points are spread far apart).\n",
    "* Distances between points become less meaningful.\n",
    "* Algorithms that rely on distance or density (like **KNN**) struggle.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why it happens?**\n",
    "\n",
    "* In high dimensions, the **volume of space grows exponentially**.\n",
    "* To cover the same proportion of the space, you’d need exponentially more data.\n",
    "* Distances between nearest and farthest neighbors tend to become **almost the same** → making it difficult to distinguish which points are actually “close.”\n",
    "\n",
    "---\n",
    "\n",
    "### **How it affects KNN performance**\n",
    "\n",
    "Since **KNN relies on distance** to find the “nearest” neighbors, the curse of dimensionality causes:\n",
    "\n",
    "1. **Reduced distance contrast**:\n",
    "\n",
    "   * In high dimensions, nearest and farthest neighbors look equally distant.\n",
    "   * Example: In 2D, you can clearly see which points are close. In 100D, distances flatten out.\n",
    "\n",
    "2. **Overfitting risk**:\n",
    "\n",
    "   * With sparse data, KNN may pick up noise instead of true patterns.\n",
    "\n",
    "3. **Increased computation cost**:\n",
    "\n",
    "   * More features → more distance calculations → slower KNN.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example (Intuition)**\n",
    "\n",
    "* Imagine points in 1D (a line): easy to say which are close.\n",
    "* In 2D (a square): still manageable.\n",
    "* In 100D: almost every point seems equally far → “nearest neighbor” loses meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to reduce the curse in KNN**\n",
    "\n",
    "* **Feature selection** → keep only relevant features.\n",
    "* **Dimensionality reduction** → PCA, t-SNE, autoencoders.\n",
    "* **Scaling/normalization** → ensures all features contribute fairly.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "The curse of dimensionality makes distance measures unreliable in high dimensions, which **hurts KNN’s accuracy and efficiency**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e8966-8c66-4491-8871-40287d46cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
    "feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c2551-16ce-4330-9a87-09fd50fcb59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## **Principal Component Analysis (PCA)**\n",
    "\n",
    "PCA is a **dimensionality reduction technique** used to transform high-dimensional data into a smaller set of uncorrelated variables called **principal components**.\n",
    "\n",
    "* It finds new axes (directions) in the data that capture the **maximum variance**.\n",
    "* These new axes are **linear combinations** of the original features.\n",
    "* The first principal component captures the most variance, the second captures the next most (orthogonal to the first), and so on.\n",
    "* You can keep the top *k* components to reduce dimensionality while preserving most information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps of PCA (simplified)**\n",
    "\n",
    "1. Standardize the data (so features are on the same scale).\n",
    "2. Compute the **covariance matrix** of the data.\n",
    "3. Find **eigenvalues and eigenvectors** of the covariance matrix.\n",
    "4. Eigenvectors = new axes (principal components).\n",
    "5. Project the data onto top *k* principal components.\n",
    "\n",
    "---\n",
    "\n",
    "## **PCA vs. Feature Selection**\n",
    "\n",
    "| Aspect                 | **PCA (Dimensionality Reduction)**                                                       | **Feature Selection**                              |\n",
    "| ---------------------- | ---------------------------------------------------------------------------------------- | -------------------------------------------------- |\n",
    "| **Definition**         | Creates new features (principal components) as linear combinations of original features. | Selects a subset of the original features.         |\n",
    "| **Nature of Features** | Transformed features (not directly interpretable).                                       | Original features (easy to interpret).             |\n",
    "| **Goal**               | Reduce dimensionality while retaining variance (information).                            | Remove irrelevant or redundant features.           |\n",
    "| **Type**               | **Feature extraction** (creates new features).                                           | **Feature selection** (keeps existing ones).       |\n",
    "| **Interpretability**   | Harder, since new components are combinations.                                           | Easier, since selected features are original ones. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "\n",
    "* Suppose you have features: Height, Weight, Arm length, Leg length.\n",
    "* **Feature Selection**: Might keep only *Height* and *Weight* if they are most informative.\n",
    "* **PCA**: Would create new features like *PC1 = 0.7(Height) + 0.6(Weight) + …* that captures maximum variance.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "\n",
    "* **PCA = feature extraction** (creates new combined features).\n",
    "* **Feature selection = keeps the most useful original features**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3615f2d7-189e-47ad-bd3e-79918c6f749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
    "important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8bc847-57c7-4588-815c-ae4f077c9710",
   "metadata": {},
   "outputs": [],
   "source": [
    "## **Eigenvalues and Eigenvectors in PCA**\n",
    "\n",
    "When we perform PCA, we compute the **covariance matrix** of the data.\n",
    "\n",
    "* The **eigenvectors** of this covariance matrix give the **directions** (axes) of the new feature space (the principal components).\n",
    "* The **eigenvalues** tell us how much **variance (information)** is captured along each eigenvector.\n",
    "\n",
    "---\n",
    "\n",
    "### **Eigenvectors (Directions of Maximum Variance)**\n",
    "\n",
    "* Think of eigenvectors as the \"arrows\" that point in the directions where the data varies the most.\n",
    "* In PCA, each eigenvector corresponds to a **principal component**.\n",
    "* They are orthogonal (perpendicular) to each other, ensuring the new components are uncorrelated.\n",
    "\n",
    "---\n",
    "\n",
    "### **Eigenvalues (Magnitude of Variance)**\n",
    "\n",
    "* Each eigenvalue corresponds to an eigenvector.\n",
    "* It represents the **amount of variance** captured in that direction.\n",
    "* Larger eigenvalue = more important principal component.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why They’re Important in PCA**\n",
    "\n",
    "1. **Identify principal components** → eigenvectors = new axes (PC1, PC2, …).\n",
    "2. **Rank components by importance** → eigenvalues show how much variance each component explains.\n",
    "3. **Dimensionality reduction** → keep only components with the largest eigenvalues (discard low-variance ones).\n",
    "\n",
    "---\n",
    "\n",
    "### **Example (Intuition)**\n",
    "\n",
    "Imagine a 2D dataset shaped like an ellipse:\n",
    "\n",
    "* The **long axis** of the ellipse = eigenvector with the largest eigenvalue (PC1, max variance).\n",
    "* The **short axis** = eigenvector with the smaller eigenvalue (PC2, less variance).\n",
    "* PCA would likely keep PC1 if reducing to 1D, since it explains most of the spread in the data.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "\n",
    "* **Eigenvectors = directions of new axes (principal components).**\n",
    "* **Eigenvalues = how much variance each new axis explains.**\n",
    "* They are the mathematical backbone of PCA.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba7ce2-2212-4f57-9cbc-518c0beddcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5: How do KNN and PCA complement each other when applied in a single\n",
    "pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16ed32-9135-44a4-a899-c88cd3431f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## **How KNN and PCA Work Together**\n",
    "\n",
    "* **KNN** is a distance-based algorithm → it decides labels or predictions by comparing distances between points.\n",
    "* **PCA** reduces dimensionality by projecting data into fewer, more informative features.\n",
    "\n",
    "👉 Since high-dimensional data can hurt KNN (curse of dimensionality), **PCA is often used before KNN** to improve performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline: PCA + KNN**\n",
    "\n",
    "1. **Preprocess data** (scaling/normalization, handle missing values).\n",
    "2. **Apply PCA** to reduce dimensionality (keep top *k* components that explain most variance).\n",
    "3. **Run KNN** on the reduced feature space.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Helps**\n",
    "\n",
    "1. **Mitigates Curse of Dimensionality**\n",
    "\n",
    "   * PCA removes redundant/noisy features → distances in KNN become more meaningful.\n",
    "\n",
    "2. **Improves Efficiency**\n",
    "\n",
    "   * Fewer dimensions → faster distance calculations in KNN.\n",
    "\n",
    "3. **Noise Reduction**\n",
    "\n",
    "   * PCA filters out low-variance directions (often noise) → KNN works with cleaner signals.\n",
    "\n",
    "4. **Better Generalization**\n",
    "\n",
    "   * Reduces overfitting since KNN won’t rely on irrelevant features.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "\n",
    "* Suppose you have 100 features in a dataset (many are correlated, e.g., height, arm length, leg length).\n",
    "* PCA reduces it to 10 principal components that explain \\~95% of variance.\n",
    "* KNN runs on these 10 PCs →\n",
    "\n",
    "  * Faster,\n",
    "  * More accurate (distances more meaningful),\n",
    "  * Less overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "\n",
    "* **PCA = dimensionality reduction / feature extraction.**\n",
    "* **KNN = classification/regression using distance.**\n",
    "* Together → PCA makes the feature space compact & noise-free, so KNN can compute more reliable distances.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e0872-83d8-423e-a569-3a7b4456492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset:\n",
    "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
    "Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
    "scaling. Compare model accuracy in both cases.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e7e8aa7-bcb0-4d7e-82ab-4fc3ba2f9275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy WITHOUT scaling: 0.72\n",
      "KNN Accuracy WITH scaling   : 0.94\n"
     ]
    }
   ],
   "source": [
    "# Question 6: KNN on Wine Dataset with and without Feature Scaling\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# -------------------------\n",
    "# 1. KNN WITHOUT SCALING\n",
    "# -------------------------\n",
    "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_no_scale.fit(X_train, y_train)\n",
    "y_pred_no_scale = knn_no_scale.predict(X_test)\n",
    "acc_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
    "\n",
    "# -------------------------\n",
    "# 2. KNN WITH SCALING\n",
    "# -------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
    "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "# Print results\n",
    "print(\"KNN Accuracy WITHOUT scaling: {:.2f}\".format(acc_no_scale))\n",
    "print(\"KNN Accuracy WITH scaling   : {:.2f}\".format(acc_scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14827667-f298-444c-a3fa-937c5e1ad519",
   "metadata": {},
   "outputs": [],
   "source": [
    "Key Takeaway\n",
    "\n",
    "Without scaling: KNN performs poorly because features like “flavanoids” (range ~0–5) and “proline” (range ~100–1600) dominate distance calculations.\n",
    "\n",
    "With scaling: Each feature contributes equally, leading to much higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6762cda-d32a-4f6e-bf00-8f41cc517c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
    "ratio of each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2932abcf-72f0-4796-b5e8-46e4083c13c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio of each Principal Component:\n",
      "PC1: 0.3620\n",
      "PC2: 0.1921\n",
      "PC3: 0.1112\n",
      "PC4: 0.0707\n",
      "PC5: 0.0656\n",
      "PC6: 0.0494\n",
      "PC7: 0.0424\n",
      "PC8: 0.0268\n",
      "PC9: 0.0222\n",
      "PC10: 0.0193\n",
      "PC11: 0.0174\n",
      "PC12: 0.0130\n",
      "PC13: 0.0080\n",
      "\n",
      "Cumulative Explained Variance:\n",
      "[0.36198848 0.55406338 0.66529969 0.73598999 0.80162293 0.85098116\n",
      " 0.89336795 0.92017544 0.94239698 0.96169717 0.97906553 0.99204785\n",
      " 1.        ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Standardize features (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA (keep all components for analysis)\n",
    "pca = PCA(n_components=X.shape[1])\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Print explained variance ratio\n",
    "print(\"Explained Variance Ratio of each Principal Component:\")\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"PC{i+1}: {ratio:.4f}\")\n",
    "\n",
    "# Also print cumulative variance for clarity\n",
    "print(\"\\nCumulative Explained Variance:\")\n",
    "print(np.cumsum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18fb257-4349-4012-ab8c-a72b07c41fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
    "components). Compare the accuracy with the original dataset.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aadfe0d6-ef1a-42e8-9509-71ff5ab361a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy on ORIGINAL dataset: 0.94\n",
      "KNN Accuracy on PCA (2 components): 0.96\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 1. KNN on ORIGINAL dataset\n",
    "# -------------------------\n",
    "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_original.fit(X_train, y_train)\n",
    "y_pred_original = knn_original.predict(X_test)\n",
    "acc_original = accuracy_score(y_test, y_pred_original)\n",
    "\n",
    "# -------------------------\n",
    "# 2. PCA Transformation (top 2 components)\n",
    "# -------------------------\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = knn_pca.predict(X_test_pca)\n",
    "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
    "\n",
    "# Print results\n",
    "print(\"KNN Accuracy on ORIGINAL dataset: {:.2f}\".format(acc_original))\n",
    "print(\"KNN Accuracy on PCA (2 components): {:.2f}\".format(acc_pca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdada8f6-e63d-44e7-b9b0-f92a38a3b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
    "manhattan) on the scaled Wine dataset and compare the results.\n",
    "(Include your Python code and output in the code box below.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a40aed-8fd8-43ad-b03c-ac785886f077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy with Euclidean distance : 0.94\n",
      "KNN Accuracy with Manhattan distance : 0.98\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 1. KNN with EUCLIDEAN distance (p=2)\n",
    "# -------------------------\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "knn_euclidean.fit(X_train, y_train)\n",
    "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
    "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
    "\n",
    "# -------------------------\n",
    "# 2. KNN with MANHATTAN distance (p=1)\n",
    "# -------------------------\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=1)\n",
    "knn_manhattan.fit(X_train, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
    "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
    "\n",
    "# Print results\n",
    "print(\"KNN Accuracy with Euclidean distance : {:.2f}\".format(acc_euclidean))\n",
    "print(\"KNN Accuracy with Manhattan distance : {:.2f}\".format(acc_manhattan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c8766-4d59-4564-9d22-87b326be0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 10: You are working with a high-dimensional gene expression dataset to\n",
    "classify patients with different types of cancer.\n",
    "Due to the large number of features and a small number of samples, traditional models\n",
    "overfit.\n",
    "Explain how you would:\n",
    "● Use PCA to reduce dimensionality\n",
    "● Decide how many components to keep\n",
    "● Use KNN for classification post-dimensionality reduction\n",
    "● Evaluate the model\n",
    "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
    "biomedical data\n",
    "(Include your Python code and output in the code box below.)\n",
    "                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b4266-784c-4726-9f13-7a2c6d497ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 1: Use PCA to Reduce Dimensionality\n",
    "\n",
    "Gene expression datasets often have thousands of features but only tens or hundreds of samples.\n",
    "\n",
    "PCA helps by extracting the directions (principal components) that capture the most variance, reducing noise and redundancy.\n",
    "\n",
    "Step 2: Decide How Many Components to Keep\n",
    "\n",
    "Use explained variance ratio to decide the number of components: keep enough PCs to retain ~90–95% variance.\n",
    "\n",
    "This reduces dimensionality but keeps most biological signal.\n",
    "\n",
    "Step 3: Use KNN for Classification Post-PCA\n",
    "\n",
    "KNN can now operate in a low-dimensional space, making distance-based classification reliable and reducing overfitting.\n",
    "\n",
    "Step 4: Evaluate the Model\n",
    "\n",
    "Use train-test split or cross-validation.\n",
    "\n",
    "Metrics: accuracy, precision, recall, F1-score, and confusion matrix, depending on clinical relevance.\n",
    "\n",
    "Step 5: Justify to Stakeholders\n",
    "\n",
    "PCA reduces thousands of noisy genes to meaningful components → improves generalization.\n",
    "\n",
    "KNN is interpretable (patients classified based on similarity to existing patients).\n",
    "\n",
    "Pipeline mitigates overfitting, improves reproducibility, and maintains biologically meaningful patterns.\n",
    "\n",
    "Python Code Example\n",
    "# Example pipeline: PCA + KNN on a simulated high-dimensional gene expression dataset\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Simulate high-dimensional gene expression data\n",
    "# 200 samples, 1000 features, 3 classes\n",
    "X, y = make_classification(n_samples=200, n_features=1000, n_informative=50, \n",
    "                           n_redundant=50, n_classes=3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee217ac-9b78-4c7c-8be9-3a3507cb024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature dimension: 1000\n",
      "Reduced feature dimension: 124\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.57      0.51        21\n",
      "           1       0.44      0.55      0.49        20\n",
      "           2       0.56      0.26      0.36        19\n",
      "\n",
      "    accuracy                           0.47        60\n",
      "   macro avg       0.49      0.46      0.45        60\n",
      "weighted avg       0.48      0.47      0.45        60\n",
      "\n",
      "Confusion Matrix:\n",
      " [[12  8  1]\n",
      " [ 6 11  3]\n",
      " [ 8  6  5]]\n",
      "\n",
      "5-fold CV Accuracy: 0.39 ± 0.06\n"
     ]
    }
   ],
   "source": [
    "# Example pipeline: PCA + KNN on a simulated high-dimensional gene expression dataset\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Simulate high-dimensional gene expression data\n",
    "# 200 samples, 1000 features, 3 classes\n",
    "X, y = make_classification(n_samples=200, n_features=1000, n_informative=50, \n",
    "                           n_redundant=50, n_classes=3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# PCA for dimensionality reduction\n",
    "# -------------------------\n",
    "pca = PCA(n_components=0.95)  # retain 95% variance\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(\"Original feature dimension:\", X_train.shape[1])\n",
    "print(\"Reduced feature dimension:\", X_train_pca.shape[1])\n",
    "\n",
    "# -------------------------\n",
    "# KNN classification\n",
    "# -------------------------\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_pca, y_train)\n",
    "y_pred = knn.predict(X_test_pca)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Optional: cross-validation for robustness\n",
    "cv_scores = cross_val_score(knn, X_train_pca, y_train, cv=5)\n",
    "print(\"\\n5-fold CV Accuracy: {:.2f} ± {:.2f}\".format(cv_scores.mean(), cv_scores.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184721d1-0e8e-4723-a49d-ddc2fa2d832d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
